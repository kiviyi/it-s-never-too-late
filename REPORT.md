# Отчёт по лабораторной работе
## Генеративные текстовые нейросети

### Студенты: 

| ФИО           | Группа      | Роль в проекте              | Оценка       |
|---------------|-------------|-----------------------------|--------------|
| Тулин Иван    | М8О-401Б-21 | RNN, отчёт                  |              |
| Ворошилов Кирилл | М8О-401Б-21 | LSTM, Двунаправленная LSTM, отчёт                 |              |

> *Комментарии проверяющего*

## RNN

Для обучения рекуррентной нейронной сети был использован роман Рея Брэдбери "451 градус по Фаренгейту", который предоставил обширный текстовый корпус для анализа и моделирования последовательностей. Процесс токенизации текста был выполнен на уровне символов, что позволило модели учитывать тонкие детали и структуру текста на наиболее низком уровне.

Сама модель состояла из нескольких ключевых компонентов. В первую очередь, использовался слой эмбединга, который преобразовывал каждый символ в числовое представление, способное захватить скрытые связи между разными символами. Затем следовал рекуррентный слой GRU (Gated Recurrent Unit), который был основным элементом для обработки последовательной информации, позволяя модели эффективно "запоминать" предыдущие символы и использовать эту информацию для предсказания следующего. Заключительным элементом архитектуры модели был полносвязный слой, который выполнял окончательную обработку данных и генерировал предсказания.

Особое внимание было уделено управлению скрытым состоянием (hidden), которое играло важную роль в сохранении информации между батчами данных. Это позволило модели поддерживать последовательность и связь между различными частями текста, даже если они были обработаны в разных батчах.

Обучение модели проходило в течение 10 эпох, в ходе которых сеть многократно проходила через весь текст, оптимизируя свои параметры для улучшения качества предсказаний.

## LSTM

Для обучения модели Long Short-Term Memory (LSTM) так же был выбран роман "451 градус по Фаренгейту". Этот текст был использован для обучения модели с применением токенизации на уровне слов, что позволило значительно повысить качество и когерентность создаваемого текста. Токенизация на уровне слов обеспечила более глубокое понимание контекста и структуры языка, что в свою очередь способствовало улучшению способности модели генерировать текст, который лучше соответствует исходному произведению.

Процесс обучения модели включал в себя 20 эпох, что позволило модели достаточно хорошо освоить особенности стиля и содержания романа. Во время обучения каждая итерация основывалась на анализе предыдущих слов и контекста, что позволяло модели адаптироваться к различным нюансам текста. 

В процессе генерации текста каждое последующее слово выбиралось случайным образом из пяти наиболее вероятных кандидатов, которые следовали за текущим текстом. Этот метод позволял сохранить определенную степень разнообразия и неожиданности в создаваемом тексте, при этом сохраняя его логичность и связь с исходным материалом.

В качестве примера работы модели приведён отрывок текста, состоящий из 100 слов, который был сгенерирован на основе начального слова "Начнем". Этот пример иллюстрирует, каким образом модель может создавать новые текстовые фрагменты, опираясь на стиль и тематические особенности исходного романа.

```
Начнем что ночью он вздрогнул « обществом » , а может быть , кожей лица и тыльной стороны ладоней он именно в этом месте ощущал некое потепление воздуха , ибо невидимка одним своим присутствием мог на пять-шесть градусов поднять температуру окружающей его атмосферы , на мир , как пустота , на определенные аминокислоты . – она погремела в пригоршне каштанами , чтобы коснуться щеки . монтаг , дрожа , как на мир , а глаза , а целых трех ? – и перескок он . – но многие же боятся . они просто ушли , чтобы искрой зажигателя воспламенить керосин .
```

Текст практически не имеет смысла, что может быть связано с недостаточным объемом обучающего материала. Вероятно, увеличение объема данных могло бы улучшить качество сгенерированного текста. Однако из-за ограничений вычислительных ресурсов обучение модели на более крупных текстовых массивах становится непрактичным. Например, обучение модели на основе одного романа в течение 100 эпох заняло около 2 часов, что уже является значительным временем для выполнения вычислительных задач.


## Двунаправленная LSTM

Для тренировки двунаправленной модели LSTM был выбран роман Брета Истона Эллиса «Американский психопат», доступный на платформе Wikibooks. В целях улучшения качества генерируемого текста была применена токенизация на уровне слов. Модель прошла обучение в течение 20 эпох, при этом каждое следующее слово выбиралось случайным образом из пяти наиболее вероятных кандидатов, которые могли следовать за уже существующим текстом. В качестве примера работы модели представлен отрывок из 100 слов, сгенерированный на основе начальной фразы `I am`:

```
I am by Tony Banks on the negative effects of television On the other hand Heathaze is a song I just dont understand while Please Dont Ask is a touching love song written to a separated wife who regains custody of the couples child Has the negative aspect of divorce ever been rendered in more intimate terms by a rock n roll group I dont think so Duke Travels and Dukes End might mean something but since the lyrics arent printed its hard to tell what Collins is singing about though thereiscomplex gorgeous piano work by Tony Banks on the latter track
```

В целом, полученный текст соответствует грамматическим правилам английского языка, однако он практически лишён смысла. Это, вероятно, связано с недостаточным объёмом текста, использованного для обучения модели. С другой стороны, из-за ограничений вычислительных ресурсов обучение модели на более крупных текстовых массивах оказывается непрактичным. Например, даже обучение на одном романе в течение 20 эпох заняло целых 3 часа, что подчеркивает значительные затраты времени при работе с большими данными.


## Вывод

В ходе лабораторной работы мы написали архитектуру для текстовых генеративных моделей. В итоге тексты имели разный уровень чистаемости их чего можно сделать следующие выводы: 
В модели Simple RNN токенизация на уровне символов позволяет учитывать тонкие детали текста, но это может не всегда быть достаточно эффективным для генерации осмысленного текста. 
В моделях LSTM и двунаправленной LSTM токенизация на уровне слов показала улучшение качества генерируемого текста, что подчеркивает важность выбора уровня токенизации в зависимости от задачи.
