# Отчёт по лабораторной работе
## Генеративные текстовые нейросети

### Студенты: 

| ФИО           | Группа      | Роль в проекте              | Оценка       |
|---------------|-------------|-----------------------------|--------------|
| Тулин Иван    | М8О-401Б-21 | RNN, отчёт                  |              |
| Ворошилов Кирилл | М8О-401Б-21 | LSTM, Двунаправленная LSTM, отчёт                 |              |

> *Комментарии проверяющего*

## RNN

## LSTM

Для обучения модели Long Short-Term Memory (LSTM) был выбран знаменитый роман Рэя Брэдбери под названием "451 градус по Фаренгейту". Этот текст был использован для обучения модели с применением токенизации на уровне слов, что позволило значительно повысить качество и когерентность создаваемого текста. Токенизация на уровне слов обеспечила более глубокое понимание контекста и структуры языка, что в свою очередь способствовало улучшению способности модели генерировать текст, который лучше соответствует исходному произведению.

Процесс обучения модели включал в себя 100 эпох, что позволило модели достаточно хорошо освоить особенности стиля и содержания романа. Во время обучения каждая итерация основывалась на анализе предыдущих слов и контекста, что позволяло модели адаптироваться к различным нюансам текста. 

В процессе генерации текста каждое последующее слово выбиралось случайным образом из пяти наиболее вероятных кандидатов, которые следовали за текущим текстом. Этот метод позволял сохранить определенную степень разнообразия и неожиданности в создаваемом тексте, при этом сохраняя его логичность и связь с исходным материалом.

В качестве примера работы модели приведён отрывок текста, состоящий из 100 слов, который был сгенерирован на основе начального слова "Начнем". Этот пример иллюстрирует, каким образом модель может создавать новые текстовые фрагменты, опираясь на стиль и тематические особенности исходного романа.

```
Начнем что ночью он вздрогнул « обществом » , а может быть , кожей лица и тыльной стороны ладоней он именно в этом месте ощущал некое потепление воздуха , ибо невидимка одним своим присутствием мог на пять-шесть градусов поднять температуру окружающей его атмосферы , на мир , как пустота , на определенные аминокислоты . – она погремела в пригоршне каштанами , чтобы коснуться щеки . монтаг , дрожа , как на мир , а глаза , а целых трех ? – и перескок он . – но многие же боятся . они просто ушли , чтобы искрой зажигателя воспламенить керосин .
```

Текст практически не имеет смысла, что может быть связано с недостаточным объемом обучающего материала. Вероятно, увеличение объема данных могло бы улучшить качество сгенерированного текста. Однако из-за ограничений вычислительных ресурсов обучение модели на более крупных текстовых массивах становится непрактичным. Например, обучение модели на основе одного романа в течение 100 эпох заняло около 2 часов, что уже является значительным временем для выполнения вычислительных задач.


## Двунаправленная LSTM

Для тренировки двунаправленной модели LSTM был выбран роман Брета Истона Эллиса «Американский психопат», доступный на платформе Wikibooks. В целях улучшения качества генерируемого текста была применена токенизация на уровне слов. Модель прошла обучение в течение 20 эпох, при этом каждое следующее слово выбиралось случайным образом из пяти наиболее вероятных кандидатов, которые могли следовать за уже существующим текстом. В качестве примера работы модели представлен отрывок из 100 слов, сгенерированный на основе начальной фразы `I am`:

```
I am by Tony Banks on the negative effects of television On the other hand Heathaze is a song I just dont understand while Please Dont Ask is a touching love song written to a separated wife who regains custody of the couples child Has the negative aspect of divorce ever been rendered in more intimate terms by a rock n roll group I dont think so Duke Travels and Dukes End might mean something but since the lyrics arent printed its hard to tell what Collins is singing about though thereiscomplex gorgeous piano work by Tony Banks on the latter track
```

В целом, полученный текст соответствует грамматическим правилам английского языка, однако он практически лишён смысла. Это, вероятно, связано с недостаточным объёмом текста, использованного для обучения модели. С другой стороны, из-за ограничений вычислительных ресурсов обучение модели на более крупных текстовых массивах оказывается непрактичным. Например, даже обучение на одном романе в течение 20 эпох заняло целых 3 часа, что подчеркивает значительные затраты времени при работе с большими данными.


## Вывод
