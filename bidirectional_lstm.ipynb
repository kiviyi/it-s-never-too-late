{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Двунаправленная LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем необходимые зависимости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим текст для обучения из txt файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('american_psycho.txt', 'r', encoding='windows-1251') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text = re.sub(r'[^a-zA-Z\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ive been a big Genesis fan ever since the release of their  albumDukeBefore that I didnt really understand any of their work though on their last album of the s the conceptladenAnd Then There Were Threea reference to band member Peter Gabriel who left the group to start a lame solo career I did enjoy the lovely Follow You Follow Me Otherwise all the albums beforeDukeseemed too artsy too intelleotual It wasDukeAtlantic  where Phil Collins presence became more apparent and the music got more modern the drum machine became more prevalent and the lyrics started getting less mystical and more specific maybe because of Peter Gabriels departure and complex ambiguous studies of loss became instead smashing firstrate pop songs that I gratefully embraced The songs themselves seemed arranged more around Collins drumming than Mike Rutherfords bass lines or Tony Banks keyboard riffs A classic example of this is Misunderstandingwhich not only was the groups first big hit of the eighties but also seemed to set the tone for the rest of theiralbums as the decade progressed The other standout onDukeis Turn It On Again which is about the negative effects of television On the other hand Heathaze is a song I just dont understand while Please Dont Ask is a touching love song written to a separated wife who regains custody of the couples child Has the negative aspect of divorce ever been rendered in more intimate terms by a rock n roll group I dont think so Duke Travels and Dukes End might mean something but since the lyrics arent printed its hard to tell what Collins is singing about though thereiscomplex gorgeous piano work by Tony Banks on the latter track The only bummer about Dukeis Alone Tonight which is way too reminiscent of Tonight Tonight Tonight from the groups later masterpieceInvisible Touchand the only example really of where Collins has plagiarized himself\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим токенизатор и словарь слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315\n",
      "Length of inputs: 265\n",
      "Length of targets: 265\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "print(len(tokens))\n",
    "\n",
    "# Создание словаря\n",
    "word_dict = {word: idx + 1 for idx, word in enumerate(set(tokens))}\n",
    "reverse_word_dict = {v: k for k, v in word_dict.items()}\n",
    "\n",
    "sequences = [word_dict[word] for word in tokens]\n",
    "sequence_length = 50\n",
    "batch_size = 32\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for i in range(len(sequences) - sequence_length):\n",
    "    inputs.append(sequences[i:i + sequence_length])\n",
    "    targets.append(sequences[i + sequence_length])\n",
    "\n",
    "print(f'Length of inputs: {len(inputs)}')\n",
    "print(f'Length of targets: {len(targets)}')\n",
    "# Преобразование списков в тензоры\n",
    "inputs_tensor = torch.tensor(inputs, dtype=torch.long)\n",
    "targets_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "# Создание TensorDataset и DataLoader\n",
    "dataset = TensorDataset(inputs_tensor, targets_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Определение собственного Dataset\n",
    "# class TextDataset(Dataset):\n",
    "#     def __init__(self, inputs, targets):\n",
    "#         self.inputs = inputs\n",
    "#         self.targets = targets\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.inputs)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return torch.tensor(self.inputs[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим обучающие примеры"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим и обучим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_units, output_dim):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_units, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_units * 2, output_dim)  # Умножаем на 2, так как LSTM двунаправленный\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        out = lstm_out[:, -1, :]  # Используем выход последнего временного шага\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMModel(vocab_size=len(word_dict) + 1, embedding_dim=embedding_dim, hidden_units=hidden_units, output_dim=len(word_dict) + 1)\n",
    "\n",
    "# Функция потерь и оптимизатор\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Создание DataLoader\n",
    "# dataset = TextDataset(sequences, targets)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 3.1059, Accuracy: 0.3057\n",
      "Epoch [2/10], Loss: 2.8098, Accuracy: 0.4151\n",
      "Epoch [3/10], Loss: 2.5258, Accuracy: 0.5434\n",
      "Epoch [4/10], Loss: 2.2095, Accuracy: 0.7358\n",
      "Epoch [5/10], Loss: 1.9266, Accuracy: 0.8604\n",
      "Epoch [6/10], Loss: 1.7079, Accuracy: 0.8981\n",
      "Epoch [7/10], Loss: 1.4358, Accuracy: 0.9547\n",
      "Epoch [8/10], Loss: 1.2146, Accuracy: 0.9585\n",
      "Epoch [9/10], Loss: 1.0381, Accuracy: 0.9774\n",
      "Epoch [10/10], Loss: 0.8742, Accuracy: 0.9887\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_inputs, batch_targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_inputs)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Подсчет точности\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_targets.size(0)\n",
    "            correct += (predicted == batch_targets).sum().item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = correct / total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Запуск обучения\n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сгенерировать текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def generate_text(model, start_text, num_words, word_dict, reverse_word_dict, sequence_length):\n",
    "    model.eval()  # Устанавливаем модель в режим оценки\n",
    "\n",
    "    # Преобразование начального текста в числовую последовательность\n",
    "    tokens = word_tokenize(start_text.lower())  # Приведение текста к нижнему регистру\n",
    "    token_ids = [word_dict.get(token, 0) for token in tokens]  # Используем 0 для неизвестных слов\n",
    "\n",
    "    # Убедимся, что у нас есть достаточно токенов для модели\n",
    "    if len(token_ids) < sequence_length:\n",
    "        token_ids = [0] * (sequence_length - len(token_ids)) + token_ids\n",
    "\n",
    "    # Перенос на устройство (CPU или GPU)\n",
    "    token_ids = torch.tensor(token_ids, dtype=torch.long).unsqueeze(0) # Добавляем размерность батча\n",
    "\n",
    "    generated_tokens = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_words):\n",
    "            # Получаем предсказания модели\n",
    "            outputs = model(token_ids)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Добавляем предсказанный токен к последовательности\n",
    "            next_token_id = predicted.item()\n",
    "            generated_tokens.append(next_token_id)\n",
    "\n",
    "            # Обновляем последовательность для следующего шага\n",
    "            token_ids = torch.cat([token_ids[:, 1:], torch.tensor([[next_token_id]], dtype=torch.long)], dim=1)\n",
    "\n",
    "    # Преобразование числовых токенов обратно в текст\n",
    "    generated_text = ' '.join(reverse_word_dict.get(token_id, '<unk>') for token_id in generated_tokens)\n",
    "    return generated_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am by Tony Banks on the negative effects of television On the other hand Heathaze is a song I just dont understand while Please Dont Ask is a touching love song written to a separated wife who regains custody of the couples child Has the negative aspect of divorce ever been rendered in more intimate terms by a rock n roll group I dont think so Duke Travels and Dukes End might mean something but since the lyrics arent printed its hard to tell what Collins is singing about though thereiscomplex gorgeous piano work by Tony Banks on the latter track\n"
     ]
    }
   ],
   "source": [
    "# Пример использования функции\n",
    "start_text = \"I am\"\n",
    "num_words = 100  # Количество слов для генерации\n",
    "generated_text = generate_text(model, start_text, num_words, word_dict, reverse_word_dict, sequence_length)\n",
    "print(start_text, generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
